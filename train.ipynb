{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import senet\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from utils import TransformImage\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "DATA_DIR = \"/home/krf/dataset/BALL/\"\n",
    "traindir = DATA_DIR + \"train\"\n",
    "valdir = DATA_DIR +\"val\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "BATCH_SIZE = 2\n",
    "WORKERS = 4\n",
    "EPOCHS = 20\n",
    "PRINT_FREQ = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  senet.se_resnext50_32x4d(num_classes = 2)\n",
    "#通过随机变化来进行数据增强\n",
    "train_tf  = TransformImage(\n",
    "    model,\n",
    "    scale=1,\n",
    "    random_crop=False,\n",
    "    random_hflip=True,\n",
    "    random_vflip=True,\n",
    "    random_rotate=True,\n",
    "    preserve_aspect_ratio=True\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.ImageFolder(traindir, transforms.Compose([\n",
    "# #         transforms.RandomSizedCrop(max(model.input_size)),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         normalize,\n",
    "#     ])),\n",
    "    datasets.ImageFolder(traindir,train_tf),\n",
    "    batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "\n",
    "val_tf = TransformImage(\n",
    "    model,\n",
    "    scale=1,\n",
    "    preserve_aspect_ratio=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(valdir,val_tf),\n",
    "    batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch,scheduler):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "#     end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "#         data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input.float())\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        #print(output.data)\n",
    "        # TP    predict 和 label 同时为1\n",
    "#         _, pred = output.topk(1, 1, True, True)\n",
    "#         pred = pred.t()\n",
    "#         print(pred,target.data)\n",
    "#         #correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "#         TP += ((pred == 1) & (target.data == 1)).cpu().numpy().sum()\n",
    "#         # TN    predict 和 label 同时为0\n",
    "#         TN += ((pred == 0) & (target.data == 0)).cpu().numpy().sum()\n",
    "#         # FN    predict 0 label 1\n",
    "#         FN += ((pred == 0) & (target.data == 1)).cpu().numpy().sum()\n",
    "#         # FP    predict 1 label 0\n",
    "#         FP += ((pred == 1) & (target.data == 0)).cpu().numpy().sum()\n",
    "#         print(TP,FP,TN,FN)\n",
    "        \n",
    "#         P = TP / (TP + FP)\n",
    "#         #print(P)\n",
    "#         R = TP / (TP + FN)\n",
    "#         F1 = 2 * R * P / (R + P)\n",
    "#         Acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        #print(F1)               \n",
    "#         # measure accuracy and record loss\n",
    "#         prec1= accuracy(output.data, target)\n",
    "#         losses.update(loss.data[0], input.size(0))\n",
    "#         top1.update(prec1[0], input.size(0))\n",
    "\n",
    "\n",
    "#         # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         # measure elapsed time\n",
    "#         batch_time.update(time.time() - end)\n",
    "#         end = time.time()\n",
    "        meters = trainMeter.update(output,target,loss,input.size(0))\n",
    "\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss:.5f}\\t'\n",
    "                  'Acc {Acc:.5f}\\t'\n",
    "                  'Precision {P:.5f}\\t'\n",
    "                  'Recall {R:.5f}\\t'\n",
    "                  'F1 {F1:.5f}'.format(\n",
    "                   epoch,i, len(train_loader), loss=meters[4],\n",
    "                   Acc=meters[3],P=meters[0],R=meters[1],F1=meters[2]))\n",
    "            \n",
    "            step = epoch*len(train_loader) + i\n",
    "           \n",
    "            writer.add_scalar('TRAIN/Precision', meters[0], step)\n",
    "            writer.add_scalar('TRAIN/Recall', meters[1], step)\n",
    "            writer.add_scalar('TRAIN/F1', meters[2], step)\n",
    "            writer.add_scalar('TRAIN/Acc', meters[3], step)\n",
    "            writer.add_scalar('TRAIN/loss',meters[4], step)\n",
    "            \n",
    "    scheduler.step(meters[4])\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion,epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "#     end = time.time()\n",
    "    meters = []\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda()\n",
    "        input = input.cuda()\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        meters = valMeter.update(output,target,loss,input.size(0))\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Loss {loss:.5f}\\t'\n",
    "                  'Acc {Acc:.5f}\\t'\n",
    "                  'Precision {P:.5f}\\t'\n",
    "                  'Recall {R:.5f}\\t'\n",
    "                  'F1 {F1:.5f}'.format(\n",
    "                   i, len(val_loader), loss=meters[4],\n",
    "                   Acc=meters[3],P=meters[0],R=meters[1],F1=meters[2]))\n",
    "            \n",
    "            step = epoch * len(val_loader) + i\n",
    "            writer.add_scalar('VAL/Precision', meters[0], step)\n",
    "            writer.add_scalar('VAL/Recall', meters[1], step)\n",
    "            writer.add_scalar('VAL/F1', meters[2], step)\n",
    "            writer.add_scalar('VAL/Acc', meters[3], step)\n",
    "            writer.add_scalar('VAL/loss',meters[4], step)\n",
    "    print(' * Acc {Acc:.5f} F1 {F1:.5f}'\n",
    "          .format(Acc=meters[3],F1=meters[2]))\n",
    "    writer.add_scalar('VAL/EPOCH_F1', meters[2], epoch)\n",
    "    return meters[2]\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "class ModelMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.losses = AverageMeter()\n",
    "        self.top1 = AverageMeter()\n",
    "        self.TP = 0\n",
    "        self.TN = 0\n",
    "        self.FN = 0\n",
    "        self.FP = 0\n",
    "        self.P=0\n",
    "        self.R=0\n",
    "        self.F1=0\n",
    "        self.Acc=0\n",
    "\n",
    "    def update(self, output,target,loss, n=1):\n",
    "        _, pred = output.data.topk(1, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        #print(pred,target.data)\n",
    "        # TP    predict 和 label 同时为1\n",
    "        self.TP += ((pred == 1) & (target.data == 1)).cpu().numpy().sum()\n",
    "        # TN    predict 和 label 同时为0\n",
    "        self.TN += ((pred == 0) & (target.data == 0)).cpu().numpy().sum()\n",
    "        # FN    predict 0 label 1\n",
    "        self.FN += ((pred == 0) & (target.data == 1)).cpu().numpy().sum()\n",
    "        # FP    predict 1 label 0\n",
    "        self.FP += ((pred == 1) & (target.data == 0)).cpu().numpy().sum()\n",
    "        #print(self.TP,self.TN,self.FN,self.FP)\n",
    "        self.P = self.TP / (self.TP + self.FP)\n",
    "        self.R = self.TP / (self.TP + self.FN)\n",
    "        self.F1 = 2 * self.R * self.P / (self.R + self.P)\n",
    "        \n",
    "        self.Acc = (self.TP + self.TN) / (self.TP + self.TN + self.FP + self.FN)\n",
    "        \n",
    "        self.losses.update(loss.data[0],n)\n",
    "\n",
    "        return [self.P,self.R,self.F1,self.Acc,self.losses.avg]\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "# def adjust_learning_rate(optimizer, epoch):\n",
    "#     \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "#     lr = args.lr * (0.1 ** (epoch // 30))\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:151: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/4443]\tLoss 0.57152\tAcc 1.00000\tPrecision 1.00000\tRecall 1.00000\tF1 1.00000\n",
      "Epoch: [0][10/4443]\tLoss 0.75224\tAcc 0.77273\tPrecision 0.50000\tRecall 0.20000\tF1 0.28571\n",
      "Epoch: [0][20/4443]\tLoss 0.78896\tAcc 0.71429\tPrecision 0.33333\tRecall 0.09091\tF1 0.14286\n",
      "Epoch: [0][30/4443]\tLoss 0.75184\tAcc 0.66129\tPrecision 0.47059\tRecall 0.40000\tF1 0.43243\n",
      "Epoch: [0][40/4443]\tLoss 0.67837\tAcc 0.70732\tPrecision 0.52632\tRecall 0.40000\tF1 0.45455\n",
      "Epoch: [0][50/4443]\tLoss 0.67133\tAcc 0.69608\tPrecision 0.52632\tRecall 0.31250\tF1 0.39216\n",
      "Epoch: [0][60/4443]\tLoss 0.65505\tAcc 0.71311\tPrecision 0.55000\tRecall 0.29730\tF1 0.38596\n",
      "Epoch: [0][70/4443]\tLoss 0.62662\tAcc 0.72535\tPrecision 0.55000\tRecall 0.26829\tF1 0.36066\n",
      "Epoch: [0][80/4443]\tLoss 0.62844\tAcc 0.72840\tPrecision 0.55000\tRecall 0.23913\tF1 0.33333\n",
      "Epoch: [0][90/4443]\tLoss 0.59955\tAcc 0.74725\tPrecision 0.55000\tRecall 0.22917\tF1 0.32353\n",
      "Epoch: [0][100/4443]\tLoss 0.60325\tAcc 0.74257\tPrecision 0.55000\tRecall 0.20370\tF1 0.29730\n",
      "Epoch: [0][110/4443]\tLoss 0.60317\tAcc 0.72973\tPrecision 0.59259\tRecall 0.24615\tF1 0.34783\n",
      "Epoch: [0][120/4443]\tLoss 0.61328\tAcc 0.71901\tPrecision 0.51282\tRecall 0.28986\tF1 0.37037\n",
      "Epoch: [0][130/4443]\tLoss 0.63541\tAcc 0.71756\tPrecision 0.51282\tRecall 0.26667\tF1 0.35088\n",
      "Epoch: [0][140/4443]\tLoss 0.62191\tAcc 0.72695\tPrecision 0.51282\tRecall 0.25641\tF1 0.34188\n",
      "Epoch: [0][150/4443]\tLoss 0.61943\tAcc 0.72517\tPrecision 0.54762\tRecall 0.26437\tF1 0.35659\n",
      "Epoch: [0][160/4443]\tLoss 0.61871\tAcc 0.72360\tPrecision 0.55769\tRecall 0.30526\tF1 0.39456\n",
      "Epoch: [0][170/4443]\tLoss 0.61443\tAcc 0.71930\tPrecision 0.56364\tRecall 0.30097\tF1 0.39241\n",
      "Epoch: [0][180/4443]\tLoss 0.59726\tAcc 0.72652\tPrecision 0.56140\tRecall 0.30189\tF1 0.39264\n",
      "Epoch: [0][190/4443]\tLoss 0.60991\tAcc 0.70942\tPrecision 0.54839\tRecall 0.29060\tF1 0.37989\n",
      "Epoch: [0][200/4443]\tLoss 0.60447\tAcc 0.70647\tPrecision 0.52055\tRecall 0.31405\tF1 0.39175\n",
      "Epoch: [0][210/4443]\tLoss 0.62005\tAcc 0.69668\tPrecision 0.52703\tRecall 0.29545\tF1 0.37864\n",
      "Epoch: [0][220/4443]\tLoss 0.62459\tAcc 0.68552\tPrecision 0.49438\tRecall 0.31884\tF1 0.38767\n",
      "Epoch: [0][230/4443]\tLoss 0.62333\tAcc 0.68398\tPrecision 0.50538\tRecall 0.31973\tF1 0.39167\n",
      "Epoch: [0][240/4443]\tLoss 0.61990\tAcc 0.68880\tPrecision 0.52941\tRecall 0.34615\tF1 0.41860\n",
      "Epoch: [0][250/4443]\tLoss 0.61695\tAcc 0.68924\tPrecision 0.52294\tRecall 0.35404\tF1 0.42222\n",
      "Epoch: [0][260/4443]\tLoss 0.61352\tAcc 0.69157\tPrecision 0.52294\tRecall 0.34337\tF1 0.41455\n",
      "Epoch: [0][270/4443]\tLoss 0.61786\tAcc 0.69004\tPrecision 0.53043\tRecall 0.34857\tF1 0.42069\n",
      "Epoch: [0][280/4443]\tLoss 0.61022\tAcc 0.69573\tPrecision 0.54622\tRecall 0.35714\tF1 0.43189\n",
      "Epoch: [0][290/4443]\tLoss 0.61120\tAcc 0.69588\tPrecision 0.54918\tRecall 0.35450\tF1 0.43087\n",
      "Epoch: [0][300/4443]\tLoss 0.61018\tAcc 0.69767\tPrecision 0.55469\tRecall 0.36224\tF1 0.43827\n",
      "Epoch: [0][310/4443]\tLoss 0.60862\tAcc 0.69614\tPrecision 0.55639\tRecall 0.36275\tF1 0.43917\n",
      "Epoch: [0][320/4443]\tLoss 0.59911\tAcc 0.70249\tPrecision 0.55970\tRecall 0.36232\tF1 0.43988\n",
      "Epoch: [0][330/4443]\tLoss 0.59597\tAcc 0.70544\tPrecision 0.56934\tRecall 0.36449\tF1 0.44444\n",
      "Epoch: [0][340/4443]\tLoss 0.59918\tAcc 0.70674\tPrecision 0.56115\tRecall 0.35945\tF1 0.43820\n",
      "Epoch: [0][350/4443]\tLoss 0.59809\tAcc 0.70798\tPrecision 0.56115\tRecall 0.35135\tF1 0.43213\n",
      "Epoch: [0][360/4443]\tLoss 0.59323\tAcc 0.71191\tPrecision 0.57241\tRecall 0.36245\tF1 0.44385\n",
      "Epoch: [0][370/4443]\tLoss 0.58733\tAcc 0.71698\tPrecision 0.57432\tRecall 0.36638\tF1 0.44737\n",
      "Epoch: [0][380/4443]\tLoss 0.58168\tAcc 0.71916\tPrecision 0.57432\tRecall 0.36017\tF1 0.44271\n",
      "Epoch: [0][390/4443]\tLoss 0.58011\tAcc 0.71995\tPrecision 0.57792\tRecall 0.36626\tF1 0.44836\n",
      "Epoch: [0][400/4443]\tLoss 0.57618\tAcc 0.72319\tPrecision 0.59259\tRecall 0.38095\tF1 0.46377\n",
      "Epoch: [0][410/4443]\tLoss 0.57305\tAcc 0.72749\tPrecision 0.60479\tRecall 0.38996\tF1 0.47418\n",
      "Epoch: [0][420/4443]\tLoss 0.56747\tAcc 0.73159\tPrecision 0.60694\tRecall 0.39924\tF1 0.48165\n",
      "Epoch: [0][430/4443]\tLoss 0.56656\tAcc 0.73318\tPrecision 0.61143\tRecall 0.39777\tF1 0.48198\n",
      "Epoch: [0][440/4443]\tLoss 0.55818\tAcc 0.73583\tPrecision 0.60674\tRecall 0.39852\tF1 0.48107\n",
      "Epoch: [0][450/4443]\tLoss 0.55869\tAcc 0.73392\tPrecision 0.60000\tRecall 0.39130\tF1 0.47368\n",
      "Epoch: [0][460/4443]\tLoss 0.55794\tAcc 0.73427\tPrecision 0.59677\tRecall 0.39502\tF1 0.47537\n",
      "Epoch: [0][470/4443]\tLoss 0.55855\tAcc 0.73461\tPrecision 0.59677\tRecall 0.38811\tF1 0.47034\n",
      "Epoch: [0][480/4443]\tLoss 0.55589\tAcc 0.73493\tPrecision 0.59375\tRecall 0.39175\tF1 0.47205\n",
      "Epoch: [0][490/4443]\tLoss 0.55593\tAcc 0.73523\tPrecision 0.59596\tRecall 0.39597\tF1 0.47581\n",
      "Epoch: [0][500/4443]\tLoss 0.55531\tAcc 0.73553\tPrecision 0.59606\tRecall 0.39803\tF1 0.47732\n",
      "Epoch: [0][510/4443]\tLoss 0.55181\tAcc 0.73777\tPrecision 0.59903\tRecall 0.40129\tF1 0.48062\n",
      "Epoch: [0][520/4443]\tLoss 0.55386\tAcc 0.73800\tPrecision 0.60000\tRecall 0.40000\tF1 0.48000\n",
      "Epoch: [0][530/4443]\tLoss 0.55769\tAcc 0.73635\tPrecision 0.59722\tRecall 0.40062\tF1 0.47955\n",
      "Epoch: [0][540/4443]\tLoss 0.55476\tAcc 0.73845\tPrecision 0.59817\tRecall 0.40184\tF1 0.48073\n",
      "Epoch: [0][550/4443]\tLoss 0.55190\tAcc 0.74047\tPrecision 0.60889\tRecall 0.40896\tF1 0.48929\n",
      "Epoch: [0][560/4443]\tLoss 0.54840\tAcc 0.74421\tPrecision 0.62069\tRecall 0.41983\tF1 0.50087\n",
      "Epoch: [0][570/4443]\tLoss 0.54673\tAcc 0.74606\tPrecision 0.62917\tRecall 0.42898\tF1 0.51014\n",
      "Epoch: [0][580/4443]\tLoss 0.54519\tAcc 0.74699\tPrecision 0.62963\tRecall 0.42857\tF1 0.51000\n",
      "Epoch: [0][590/4443]\tLoss 0.54513\tAcc 0.74619\tPrecision 0.62550\tRecall 0.43251\tF1 0.51140\n",
      "Epoch: [0][600/4443]\tLoss 0.54638\tAcc 0.74626\tPrecision 0.62550\tRecall 0.42663\tF1 0.50727\n",
      "Epoch: [0][610/4443]\tLoss 0.54661\tAcc 0.74632\tPrecision 0.62257\tRecall 0.42895\tF1 0.50794\n",
      "Epoch: [0][620/4443]\tLoss 0.54404\tAcc 0.74718\tPrecision 0.62835\tRecall 0.43045\tF1 0.51090\n",
      "Epoch: [0][630/4443]\tLoss 0.54094\tAcc 0.74881\tPrecision 0.63158\tRecall 0.43411\tF1 0.51455\n",
      "Epoch: [0][640/4443]\tLoss 0.54144\tAcc 0.74883\tPrecision 0.63370\tRecall 0.43797\tF1 0.51796\n",
      "Epoch: [0][650/4443]\tLoss 0.54234\tAcc 0.74885\tPrecision 0.63441\tRecall 0.44030\tF1 0.51982\n",
      "Epoch: [0][660/4443]\tLoss 0.54016\tAcc 0.74962\tPrecision 0.63287\tRecall 0.44472\tF1 0.52237\n",
      "Epoch: [0][670/4443]\tLoss 0.53852\tAcc 0.75037\tPrecision 0.63448\tRecall 0.44552\tF1 0.52347\n",
      "Epoch: [0][680/4443]\tLoss 0.53624\tAcc 0.75184\tPrecision 0.63973\tRecall 0.45131\tF1 0.52925\n",
      "Epoch: [0][690/4443]\tLoss 0.53471\tAcc 0.75253\tPrecision 0.63758\tRecall 0.44811\tF1 0.52632\n",
      "Epoch: [0][700/4443]\tLoss 0.53106\tAcc 0.75321\tPrecision 0.63667\tRecall 0.44626\tF1 0.52473\n",
      "Epoch: [0][710/4443]\tLoss 0.52748\tAcc 0.75387\tPrecision 0.63607\tRecall 0.44804\tF1 0.52575\n",
      "Epoch: [0][720/4443]\tLoss 0.52666\tAcc 0.75381\tPrecision 0.63462\tRecall 0.45103\tF1 0.52730\n",
      "Epoch: [0][730/4443]\tLoss 0.52898\tAcc 0.75308\tPrecision 0.63750\tRecall 0.45434\tF1 0.53056\n",
      "Epoch: [0][740/4443]\tLoss 0.52986\tAcc 0.75101\tPrecision 0.62805\tRecall 0.45475\tF1 0.52753\n",
      "Epoch: [0][750/4443]\tLoss 0.53314\tAcc 0.75100\tPrecision 0.62805\tRecall 0.44978\tF1 0.52417\n",
      "Epoch: [0][760/4443]\tLoss 0.53886\tAcc 0.74770\tPrecision 0.62647\tRecall 0.45319\tF1 0.52593\n",
      "Epoch: [0][770/4443]\tLoss 0.54252\tAcc 0.74643\tPrecision 0.62069\tRecall 0.45474\tF1 0.52491\n",
      "Epoch: [0][780/4443]\tLoss 0.53974\tAcc 0.74712\tPrecision 0.62069\tRecall 0.45094\tF1 0.52237\n",
      "Epoch: [0][790/4443]\tLoss 0.54127\tAcc 0.74716\tPrecision 0.62108\tRecall 0.44948\tF1 0.52153\n",
      "Epoch: [0][800/4443]\tLoss 0.54028\tAcc 0.74657\tPrecision 0.62011\tRecall 0.45122\tF1 0.52235\n",
      "Epoch: [0][810/4443]\tLoss 0.54038\tAcc 0.74723\tPrecision 0.61944\tRecall 0.44960\tF1 0.52103\n",
      "Epoch: [0][820/4443]\tLoss 0.53990\tAcc 0.74848\tPrecision 0.62050\tRecall 0.44800\tF1 0.52033\n",
      "Epoch: [0][830/4443]\tLoss 0.54006\tAcc 0.74910\tPrecision 0.62295\tRecall 0.44970\tF1 0.52234\n",
      "Epoch: [0][840/4443]\tLoss 0.53989\tAcc 0.74911\tPrecision 0.62299\tRecall 0.45331\tF1 0.52477\n",
      "Epoch: [0][850/4443]\tLoss 0.53898\tAcc 0.74912\tPrecision 0.62434\tRecall 0.45298\tF1 0.52503\n",
      "Epoch: [0][860/4443]\tLoss 0.53779\tAcc 0.74971\tPrecision 0.62368\tRecall 0.45143\tF1 0.52376\n",
      "Epoch: [0][870/4443]\tLoss 0.53582\tAcc 0.75086\tPrecision 0.62304\tRecall 0.45076\tF1 0.52308\n",
      "Epoch: [0][880/4443]\tLoss 0.53453\tAcc 0.75085\tPrecision 0.62629\tRecall 0.45251\tF1 0.52541\n",
      "Epoch: [0][890/4443]\tLoss 0.53171\tAcc 0.75196\tPrecision 0.62720\tRecall 0.45856\tF1 0.52979\n",
      "Epoch: [0][900/4443]\tLoss 0.53211\tAcc 0.75194\tPrecision 0.62687\tRecall 0.45902\tF1 0.52997\n",
      "Epoch: [0][910/4443]\tLoss 0.53279\tAcc 0.75082\tPrecision 0.62654\tRecall 0.45781\tF1 0.52905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][920/4443]\tLoss 0.53326\tAcc 0.75081\tPrecision 0.62500\tRecall 0.46181\tF1 0.53115\n",
      "Epoch: [0][930/4443]\tLoss 0.53337\tAcc 0.75081\tPrecision 0.62530\tRecall 0.46046\tF1 0.53036\n",
      "Epoch: [0][940/4443]\tLoss 0.53188\tAcc 0.75186\tPrecision 0.62911\tRecall 0.46447\tF1 0.53440\n",
      "Epoch: [0][950/4443]\tLoss 0.53126\tAcc 0.75289\tPrecision 0.63218\tRecall 0.47009\tF1 0.53922\n",
      "Epoch: [0][960/4443]\tLoss 0.53052\tAcc 0.75338\tPrecision 0.63265\tRecall 0.47208\tF1 0.54070\n",
      "Epoch: [0][970/4443]\tLoss 0.53145\tAcc 0.75283\tPrecision 0.63252\tRecall 0.47412\tF1 0.54198\n",
      "Epoch: [0][980/4443]\tLoss 0.52963\tAcc 0.75382\tPrecision 0.63297\tRecall 0.47682\tF1 0.54391\n",
      "Epoch: [0][990/4443]\tLoss 0.53342\tAcc 0.75227\tPrecision 0.63181\tRecall 0.47386\tF1 0.54155\n",
      "Epoch: [0][1000/4443]\tLoss 0.53394\tAcc 0.75275\tPrecision 0.63248\tRecall 0.47819\tF1 0.54462\n",
      "Epoch: [0][1010/4443]\tLoss 0.53440\tAcc 0.75272\tPrecision 0.63270\tRecall 0.47680\tF1 0.54380\n",
      "Epoch: [0][1020/4443]\tLoss 0.53592\tAcc 0.75220\tPrecision 0.63235\tRecall 0.47627\tF1 0.54332\n",
      "Epoch: [0][1030/4443]\tLoss 0.53563\tAcc 0.75218\tPrecision 0.63354\tRecall 0.47813\tF1 0.54497\n",
      "Epoch: [0][1040/4443]\tLoss 0.53489\tAcc 0.75216\tPrecision 0.63265\tRecall 0.47988\tF1 0.54577\n",
      "Epoch: [0][1050/4443]\tLoss 0.53579\tAcc 0.75262\tPrecision 0.63563\tRecall 0.48012\tF1 0.54704\n",
      "Epoch: [0][1060/4443]\tLoss 0.53618\tAcc 0.75212\tPrecision 0.63400\tRecall 0.48030\tF1 0.54655\n",
      "Epoch: [0][1070/4443]\tLoss 0.53447\tAcc 0.75303\tPrecision 0.63366\tRecall 0.48193\tF1 0.54748\n",
      "Epoch: [0][1080/4443]\tLoss 0.53419\tAcc 0.75347\tPrecision 0.63241\tRecall 0.47976\tF1 0.54561\n",
      "Epoch: [0][1090/4443]\tLoss 0.53239\tAcc 0.75390\tPrecision 0.63314\tRecall 0.47768\tF1 0.54453\n",
      "Epoch: [0][1100/4443]\tLoss 0.53226\tAcc 0.75386\tPrecision 0.63209\tRecall 0.47710\tF1 0.54377\n",
      "Epoch: [0][1110/4443]\tLoss 0.53140\tAcc 0.75473\tPrecision 0.63514\tRecall 0.48029\tF1 0.54697\n",
      "Epoch: [0][1120/4443]\tLoss 0.52944\tAcc 0.75602\tPrecision 0.63532\tRecall 0.48110\tF1 0.54756\n",
      "Epoch: [0][1130/4443]\tLoss 0.53021\tAcc 0.75641\tPrecision 0.63480\tRecall 0.47977\tF1 0.54650\n",
      "Epoch: [0][1140/4443]\tLoss 0.52809\tAcc 0.75723\tPrecision 0.63688\tRecall 0.47994\tF1 0.54739\n",
      "Epoch: [0][1150/4443]\tLoss 0.52799\tAcc 0.75760\tPrecision 0.63910\tRecall 0.48159\tF1 0.54927\n",
      "Epoch: [0][1160/4443]\tLoss 0.52610\tAcc 0.75883\tPrecision 0.63806\tRecall 0.48305\tF1 0.54984\n",
      "Epoch: [0][1170/4443]\tLoss 0.52653\tAcc 0.75918\tPrecision 0.63873\tRecall 0.48107\tF1 0.54880\n",
      "Epoch: [0][1180/4443]\tLoss 0.52519\tAcc 0.76037\tPrecision 0.64140\tRecall 0.48261\tF1 0.55079\n",
      "Epoch: [0][1190/4443]\tLoss 0.52797\tAcc 0.75819\tPrecision 0.63884\tRecall 0.48285\tF1 0.55000\n",
      "Epoch: [0][1200/4443]\tLoss 0.52723\tAcc 0.75895\tPrecision 0.64029\tRecall 0.48435\tF1 0.55151\n",
      "Epoch: [0][1210/4443]\tLoss 0.52852\tAcc 0.75888\tPrecision 0.64093\tRecall 0.48178\tF1 0.55008\n",
      "Epoch: [0][1220/4443]\tLoss 0.52661\tAcc 0.75962\tPrecision 0.64171\tRecall 0.48257\tF1 0.55088\n",
      "Epoch: [0][1230/4443]\tLoss 0.52537\tAcc 0.75995\tPrecision 0.64021\tRecall 0.48400\tF1 0.55125\n",
      "Epoch: [0][1240/4443]\tLoss 0.52481\tAcc 0.76027\tPrecision 0.64021\tRecall 0.48143\tF1 0.54958\n",
      "Epoch: [0][1250/4443]\tLoss 0.52297\tAcc 0.76179\tPrecision 0.64085\tRecall 0.48148\tF1 0.54985\n",
      "Epoch: [0][1260/4443]\tLoss 0.52415\tAcc 0.76090\tPrecision 0.63986\tRecall 0.47969\tF1 0.54831\n",
      "Epoch: [0][1270/4443]\tLoss 0.52297\tAcc 0.76121\tPrecision 0.63715\tRecall 0.47974\tF1 0.54735\n",
      "Epoch: [0][1280/4443]\tLoss 0.52436\tAcc 0.76190\tPrecision 0.63979\tRecall 0.48254\tF1 0.55015\n",
      "Epoch: [0][1290/4443]\tLoss 0.52382\tAcc 0.76220\tPrecision 0.64116\tRecall 0.48333\tF1 0.55117\n",
      "Epoch: [0][1300/4443]\tLoss 0.52278\tAcc 0.76326\tPrecision 0.64129\tRecall 0.48404\tF1 0.55167\n",
      "Epoch: [0][1310/4443]\tLoss 0.52132\tAcc 0.76430\tPrecision 0.64189\tRecall 0.48346\tF1 0.55152\n",
      "Epoch: [0][1320/4443]\tLoss 0.52044\tAcc 0.76533\tPrecision 0.64370\tRecall 0.48420\tF1 0.55267\n",
      "Epoch: [0][1330/4443]\tLoss 0.52232\tAcc 0.76484\tPrecision 0.64452\tRecall 0.48500\tF1 0.55350\n",
      "Epoch: [0][1340/4443]\tLoss 0.52309\tAcc 0.76435\tPrecision 0.64169\tRecall 0.48883\tF1 0.55493\n",
      "Epoch: [0][1350/4443]\tLoss 0.52056\tAcc 0.76573\tPrecision 0.64228\tRecall 0.48886\tF1 0.55517\n",
      "Epoch: [0][1360/4443]\tLoss 0.52263\tAcc 0.76561\tPrecision 0.64228\tRecall 0.48585\tF1 0.55322\n",
      "Epoch: [0][1370/4443]\tLoss 0.52180\tAcc 0.76550\tPrecision 0.64078\tRecall 0.48470\tF1 0.55192\n",
      "Epoch: [0][1380/4443]\tLoss 0.52004\tAcc 0.76647\tPrecision 0.64160\tRecall 0.48783\tF1 0.55425\n",
      "Epoch: [0][1390/4443]\tLoss 0.52094\tAcc 0.76600\tPrecision 0.64127\tRecall 0.48733\tF1 0.55380\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, verbose=True)\n",
    "\n",
    "#model = model.cuda()\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "# TP = 0,TN = 0,FN = 0, FP = 0\n",
    "writer = SummaryWriter()\n",
    "best_f1 = 0\n",
    "trainMeter = ModelMeter()\n",
    "valMeter = ModelMeter()\n",
    "for epoch in range(EPOCHS):\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch, scheduler)\n",
    "    # evaluate on validation set\n",
    "    F1 = validate(val_loader, model, criterion,epoch)\n",
    "    \n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = F1 > best_f1\n",
    "    best_f1 = max(F1, best_f1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': \"SE_ResNeXt50\",\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_f1,\n",
    "    }, is_best)\n",
    "# export scalar data to JSON for external processing\n",
    "writer.export_scalars_to_json(\"./test.json\")\n",
    "writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
