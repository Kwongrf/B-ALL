{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCNN_Alex(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): ReLU(inplace)\n",
      "    (1): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "=> loading checkpoint 'tcnn5_checkpoint-Copy1.pth.tar'\n",
      "start at epoch 800, best_f1=0.765144861579079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:223: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [800][0/278]\tLoss 0.00495\tAcc 1.00000\tPrecision 1.00000\tRecall 1.00000\tF1 1.00000\n",
      "Epoch: [800][20/278]\tLoss 0.01283\tAcc 0.99702\tPrecision 1.00000\tRecall 0.99057\tF1 0.99526\n",
      "Epoch: [800][40/278]\tLoss 0.01331\tAcc 0.99466\tPrecision 0.99215\tRecall 0.98956\tF1 0.99085\n",
      "Epoch: [800][60/278]\tLoss 0.01365\tAcc 0.99488\tPrecision 0.99310\tRecall 0.98969\tF1 0.99139\n",
      "Epoch: [800][80/278]\tLoss 0.01386\tAcc 0.99460\tPrecision 0.99332\tRecall 0.98803\tF1 0.99067\n",
      "Epoch: [800][100/278]\tLoss 0.01491\tAcc 0.99443\tPrecision 0.99353\tRecall 0.98715\tF1 0.99033\n",
      "Epoch: [800][120/278]\tLoss 0.01522\tAcc 0.99406\tPrecision 0.99191\tRecall 0.98748\tF1 0.98969\n",
      "Epoch: [800][140/278]\tLoss 0.01513\tAcc 0.99424\tPrecision 0.99175\tRecall 0.98878\tF1 0.99026\n",
      "Epoch: [800][160/278]\tLoss 0.01546\tAcc 0.99457\tPrecision 0.99207\tRecall 0.98946\tF1 0.99077\n",
      "Epoch: [800][180/278]\tLoss 0.01508\tAcc 0.99482\tPrecision 0.99292\tRecall 0.98941\tF1 0.99116\n",
      "Epoch: [800][200/278]\tLoss 0.01503\tAcc 0.99487\tPrecision 0.99311\tRecall 0.98945\tF1 0.99128\n",
      "Epoch: [800][220/278]\tLoss 0.01561\tAcc 0.99491\tPrecision 0.99325\tRecall 0.98944\tF1 0.99134\n",
      "Epoch: [800][240/278]\tLoss 0.01609\tAcc 0.99494\tPrecision 0.99290\tRecall 0.98983\tF1 0.99136\n",
      "Epoch: [800][260/278]\tLoss 0.01645\tAcc 0.99485\tPrecision 0.99345\tRecall 0.98899\tF1 0.99121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:138: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/krf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:139: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/21]\tLoss 0.08331\tAcc 0.96875\tPrecision 0.00000\tRecall 0.50000\tF1 0.00000\n",
      "Test: [20/21]\tLoss 0.60053\tAcc 0.86165\tPrecision 0.89815\tRecall 0.73485\tF1 0.80833\n",
      " * Acc 0.86165 F1 0.80833\n",
      "Epoch: [801][0/278]\tLoss 0.01662\tAcc 0.99462\tPrecision 0.99269\tRecall 0.98889\tF1 0.99079\n",
      "Epoch: [801][20/278]\tLoss 0.01618\tAcc 0.99477\tPrecision 0.99250\tRecall 0.98967\tF1 0.99108\n",
      "Epoch: [801][40/278]\tLoss 0.01609\tAcc 0.99500\tPrecision 0.99295\tRecall 0.98995\tF1 0.99145\n",
      "Epoch: [801][60/278]\tLoss 0.01553\tAcc 0.99529\tPrecision 0.99336\tRecall 0.99055\tF1 0.99195\n",
      "Epoch: [801][80/278]\tLoss 0.01571\tAcc 0.99530\tPrecision 0.99343\tRecall 0.99047\tF1 0.99195\n",
      "Epoch: [801][100/278]\tLoss 0.01522\tAcc 0.99554\tPrecision 0.99377\tRecall 0.99097\tF1 0.99237\n",
      "Epoch: [801][120/278]\tLoss 0.01520\tAcc 0.99561\tPrecision 0.99411\tRecall 0.99092\tF1 0.99251\n",
      "Epoch: [801][140/278]\tLoss 0.01511\tAcc 0.99560\tPrecision 0.99387\tRecall 0.99109\tF1 0.99248\n",
      "Epoch: [801][160/278]\tLoss 0.01527\tAcc 0.99544\tPrecision 0.99412\tRecall 0.99024\tF1 0.99218\n",
      "Epoch: [801][180/278]\tLoss 0.01554\tAcc 0.99530\tPrecision 0.99416\tRecall 0.98977\tF1 0.99196\n",
      "Epoch: [801][200/278]\tLoss 0.01598\tAcc 0.99504\tPrecision 0.99372\tRecall 0.98929\tF1 0.99150\n",
      "Epoch: [801][220/278]\tLoss 0.01629\tAcc 0.99480\tPrecision 0.99290\tRecall 0.98929\tF1 0.99109\n",
      "Epoch: [801][240/278]\tLoss 0.01647\tAcc 0.99476\tPrecision 0.99295\tRecall 0.98905\tF1 0.99099\n",
      "Epoch: [801][260/278]\tLoss 0.01668\tAcc 0.99478\tPrecision 0.99323\tRecall 0.98890\tF1 0.99106\n",
      "Test: [0/21]\tLoss 0.58206\tAcc 0.86514\tPrecision 0.88991\tRecall 0.73485\tF1 0.80498\n",
      "Test: [20/21]\tLoss 0.59643\tAcc 0.86466\tPrecision 0.89367\tRecall 0.74811\tF1 0.81443\n",
      " * Acc 0.86466 F1 0.81443\n",
      "Epoch: [802][0/278]\tLoss 0.01702\tAcc 0.99472\tPrecision 0.99307\tRecall 0.98888\tF1 0.99097\n",
      "Epoch: [802][20/278]\tLoss 0.01706\tAcc 0.99463\tPrecision 0.99274\tRecall 0.98888\tF1 0.99081\n",
      "Epoch: [802][40/278]\tLoss 0.01712\tAcc 0.99466\tPrecision 0.99299\tRecall 0.98872\tF1 0.99085\n",
      "Epoch: [802][60/278]\tLoss 0.01712\tAcc 0.99463\tPrecision 0.99271\tRecall 0.98893\tF1 0.99081\n",
      "Epoch: [802][80/278]\tLoss 0.01709\tAcc 0.99465\tPrecision 0.99295\tRecall 0.98880\tF1 0.99087\n",
      "Epoch: [802][100/278]\tLoss 0.01701\tAcc 0.99467\tPrecision 0.99284\tRecall 0.98898\tF1 0.99091\n",
      "Epoch: [802][120/278]\tLoss 0.01717\tAcc 0.99455\tPrecision 0.99290\tRecall 0.98853\tF1 0.99071\n",
      "Epoch: [802][140/278]\tLoss 0.01753\tAcc 0.99439\tPrecision 0.99250\tRecall 0.98841\tF1 0.99045\n",
      "Epoch: [802][160/278]\tLoss 0.01745\tAcc 0.99442\tPrecision 0.99241\tRecall 0.98859\tF1 0.99050\n",
      "Epoch: [802][180/278]\tLoss 0.01734\tAcc 0.99453\tPrecision 0.99263\tRecall 0.98877\tF1 0.99069\n",
      "Epoch: [802][200/278]\tLoss 0.01742\tAcc 0.99446\tPrecision 0.99253\tRecall 0.98863\tF1 0.99057\n",
      "Epoch: [802][220/278]\tLoss 0.01760\tAcc 0.99453\tPrecision 0.99273\tRecall 0.98866\tF1 0.99069\n",
      "Epoch: [802][240/278]\tLoss 0.01751\tAcc 0.99455\tPrecision 0.99263\tRecall 0.98879\tF1 0.99070\n",
      "Epoch: [802][260/278]\tLoss 0.01741\tAcc 0.99456\tPrecision 0.99267\tRecall 0.98879\tF1 0.99073\n",
      "Test: [0/21]\tLoss 0.58439\tAcc 0.86711\tPrecision 0.89165\tRecall 0.74811\tF1 0.81359\n",
      "Test: [20/21]\tLoss 0.61272\tAcc 0.86466\tPrecision 0.90031\tRecall 0.74116\tF1 0.81302\n",
      " * Acc 0.86466 F1 0.81302\n",
      "Epoch: [803][0/278]\tLoss 0.01744\tAcc 0.99453\tPrecision 0.99268\tRecall 0.98862\tF1 0.99064\n",
      "Epoch: [803][20/278]\tLoss 0.01753\tAcc 0.99451\tPrecision 0.99284\tRecall 0.98838\tF1 0.99061\n",
      "Epoch: [803][40/278]\tLoss 0.01771\tAcc 0.99449\tPrecision 0.99275\tRecall 0.98838\tF1 0.99056\n",
      "Epoch: [803][60/278]\tLoss 0.01756\tAcc 0.99455\tPrecision 0.99280\tRecall 0.98853\tF1 0.99066\n",
      "Epoch: [803][80/278]\tLoss 0.01738\tAcc 0.99460\tPrecision 0.99284\tRecall 0.98867\tF1 0.99075\n",
      "Epoch: [803][100/278]\tLoss 0.01728\tAcc 0.99468\tPrecision 0.99298\tRecall 0.98878\tF1 0.99088\n",
      "Epoch: [803][120/278]\tLoss 0.01734\tAcc 0.99466\tPrecision 0.99302\tRecall 0.98867\tF1 0.99084\n",
      "Epoch: [803][140/278]\tLoss 0.01728\tAcc 0.99474\tPrecision 0.99307\tRecall 0.98893\tF1 0.99099\n",
      "Epoch: [803][160/278]\tLoss 0.01717\tAcc 0.99478\tPrecision 0.99320\tRecall 0.98893\tF1 0.99106\n",
      "Epoch: [803][180/278]\tLoss 0.01704\tAcc 0.99488\tPrecision 0.99333\tRecall 0.98914\tF1 0.99123\n",
      "Epoch: [803][200/278]\tLoss 0.01719\tAcc 0.99480\tPrecision 0.99325\tRecall 0.98893\tF1 0.99108\n",
      "Epoch: [803][220/278]\tLoss 0.01708\tAcc 0.99487\tPrecision 0.99338\tRecall 0.98904\tF1 0.99120\n",
      "Epoch: [803][240/278]\tLoss 0.01713\tAcc 0.99479\tPrecision 0.99331\tRecall 0.98886\tF1 0.99108\n",
      "Epoch: [803][260/278]\tLoss 0.01705\tAcc 0.99483\tPrecision 0.99333\tRecall 0.98897\tF1 0.99115\n",
      "Test: [0/21]\tLoss 0.60421\tAcc 0.86630\tPrecision 0.89893\tRecall 0.74116\tF1 0.81246\n",
      "Test: [20/21]\tLoss 0.61897\tAcc 0.86391\tPrecision 0.89794\tRecall 0.74148\tF1 0.81224\n",
      " * Acc 0.86391 F1 0.81224\n",
      "Epoch: [804][0/278]\tLoss 0.01707\tAcc 0.99483\tPrecision 0.99335\tRecall 0.98896\tF1 0.99115\n",
      "Epoch: [804][20/278]\tLoss 0.01704\tAcc 0.99486\tPrecision 0.99337\tRecall 0.98906\tF1 0.99121\n",
      "Epoch: [804][40/278]\tLoss 0.01693\tAcc 0.99490\tPrecision 0.99330\tRecall 0.98926\tF1 0.99128\n",
      "Epoch: [804][60/278]\tLoss 0.01691\tAcc 0.99493\tPrecision 0.99341\tRecall 0.98925\tF1 0.99133\n",
      "Epoch: [804][80/278]\tLoss 0.01699\tAcc 0.99489\tPrecision 0.99334\tRecall 0.98916\tF1 0.99125\n",
      "Epoch: [804][100/278]\tLoss 0.01697\tAcc 0.99492\tPrecision 0.99336\tRecall 0.98924\tF1 0.99129\n",
      "Epoch: [804][120/278]\tLoss 0.01707\tAcc 0.99482\tPrecision 0.99312\tRecall 0.98916\tF1 0.99114\n",
      "Epoch: [804][140/278]\tLoss 0.01708\tAcc 0.99481\tPrecision 0.99305\tRecall 0.98915\tF1 0.99110\n",
      "Epoch: [804][160/278]\tLoss 0.01704\tAcc 0.99479\tPrecision 0.99300\tRecall 0.98916\tF1 0.99107\n",
      "Epoch: [804][180/278]\tLoss 0.01701\tAcc 0.99485\tPrecision 0.99310\tRecall 0.98924\tF1 0.99117\n",
      "Epoch: [804][200/278]\tLoss 0.01698\tAcc 0.99485\tPrecision 0.99313\tRecall 0.98924\tF1 0.99118\n",
      "Epoch: [804][220/278]\tLoss 0.01715\tAcc 0.99479\tPrecision 0.99292\tRecall 0.98925\tF1 0.99108\n",
      "Epoch: [804][240/278]\tLoss 0.01731\tAcc 0.99475\tPrecision 0.99302\tRecall 0.98901\tF1 0.99101\n",
      "Epoch: [804][260/278]\tLoss 0.01723\tAcc 0.99481\tPrecision 0.99304\tRecall 0.98917\tF1 0.99110\n",
      "Test: [0/21]\tLoss 0.61359\tAcc 0.86478\tPrecision 0.89588\tRecall 0.74148\tF1 0.81140\n",
      "Test: [20/21]\tLoss 0.62911\tAcc 0.86376\tPrecision 0.89880\tRecall 0.74015\tF1 0.81180\n",
      " * Acc 0.86376 F1 0.81180\n",
      "Epoch: [805][0/278]\tLoss 0.01729\tAcc 0.99471\tPrecision 0.99306\tRecall 0.98886\tF1 0.99096\n",
      "Epoch: [805][20/278]\tLoss 0.01725\tAcc 0.99472\tPrecision 0.99294\tRecall 0.98904\tF1 0.99099\n",
      "Epoch: [805][40/278]\tLoss 0.01722\tAcc 0.99473\tPrecision 0.99297\tRecall 0.98906\tF1 0.99101\n",
      "Epoch: [805][60/278]\tLoss 0.01719\tAcc 0.99472\tPrecision 0.99300\tRecall 0.98898\tF1 0.99099\n",
      "Epoch: [805][80/278]\tLoss 0.01721\tAcc 0.99464\tPrecision 0.99287\tRecall 0.98885\tF1 0.99086\n",
      "Epoch: [805][100/278]\tLoss 0.01720\tAcc 0.99465\tPrecision 0.99283\tRecall 0.98893\tF1 0.99087\n",
      "Epoch: [805][120/278]\tLoss 0.01724\tAcc 0.99462\tPrecision 0.99278\tRecall 0.98887\tF1 0.99082\n",
      "Epoch: [805][140/278]\tLoss 0.01719\tAcc 0.99465\tPrecision 0.99288\tRecall 0.98887\tF1 0.99087\n",
      "Epoch: [805][160/278]\tLoss 0.01722\tAcc 0.99461\tPrecision 0.99284\tRecall 0.98882\tF1 0.99082\n",
      "Epoch: [805][180/278]\tLoss 0.01727\tAcc 0.99454\tPrecision 0.99272\tRecall 0.98868\tF1 0.99070\n",
      "Epoch: [805][200/278]\tLoss 0.01734\tAcc 0.99453\tPrecision 0.99280\tRecall 0.98855\tF1 0.99067\n",
      "Epoch: [805][220/278]\tLoss 0.01743\tAcc 0.99449\tPrecision 0.99276\tRecall 0.98842\tF1 0.99059\n",
      "Epoch: [805][240/278]\tLoss 0.01745\tAcc 0.99450\tPrecision 0.99278\tRecall 0.98843\tF1 0.99060\n",
      "Epoch: [805][260/278]\tLoss 0.01738\tAcc 0.99452\tPrecision 0.99279\tRecall 0.98849\tF1 0.99064\n",
      "Test: [0/21]\tLoss 0.62356\tAcc 0.86476\tPrecision 0.89798\tRecall 0.74015\tF1 0.81146\n",
      "Test: [20/21]\tLoss 0.63659\tAcc 0.86341\tPrecision 0.90240\tRecall 0.73548\tF1 0.81043\n",
      " * Acc 0.86341 F1 0.81043\n",
      "Epoch: [806][0/278]\tLoss 0.01737\tAcc 0.99455\tPrecision 0.99280\tRecall 0.98855\tF1 0.99067\n",
      "Epoch: [806][20/278]\tLoss 0.01743\tAcc 0.99448\tPrecision 0.99276\tRecall 0.98837\tF1 0.99056\n",
      "Epoch: [806][40/278]\tLoss 0.01747\tAcc 0.99444\tPrecision 0.99267\tRecall 0.98833\tF1 0.99049\n",
      "Epoch: [806][60/278]\tLoss 0.01747\tAcc 0.99443\tPrecision 0.99263\tRecall 0.98834\tF1 0.99048\n",
      "Epoch: [806][80/278]\tLoss 0.01745\tAcc 0.99440\tPrecision 0.99265\tRecall 0.98824\tF1 0.99044\n",
      "Epoch: [806][100/278]\tLoss 0.01741\tAcc 0.99439\tPrecision 0.99268\tRecall 0.98819\tF1 0.99043\n",
      "Epoch: [806][120/278]\tLoss 0.01742\tAcc 0.99440\tPrecision 0.99270\tRecall 0.98820\tF1 0.99045\n",
      "Epoch: [806][140/278]\tLoss 0.01743\tAcc 0.99440\tPrecision 0.99266\tRecall 0.98822\tF1 0.99044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [806][160/278]\tLoss 0.01749\tAcc 0.99437\tPrecision 0.99263\tRecall 0.98818\tF1 0.99040\n",
      "Epoch: [806][180/278]\tLoss 0.01750\tAcc 0.99440\tPrecision 0.99265\tRecall 0.98824\tF1 0.99044\n",
      "Epoch: [806][200/278]\tLoss 0.01754\tAcc 0.99438\tPrecision 0.99261\tRecall 0.98819\tF1 0.99040\n"
     ]
    }
   ],
   "source": [
    "# %load train.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[1]:\n",
    "from t_cnn import tcnn\n",
    "\n",
    "model = tcnn(num_classes = 2,pretrained = True, model_root = '/home/krf/model/BALL/')\n",
    "\n",
    "import senet\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from utils import TransformImage\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "DATA_DIR = \"/home/krf/dataset/BALL/\"\n",
    "traindir = DATA_DIR + \"train\"\n",
    "valdir = DATA_DIR +\"val\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "BATCH_SIZE = 32\n",
    "WORKERS = 4\n",
    "START = 0\n",
    "EPOCHS = 1200\n",
    "PRINT_FREQ = 20\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "#model =  senet.se_resnext50_32x4d(num_classes = 2)\n",
    "#通过随机变化来进行数据增强\n",
    "train_tf  = TransformImage(\n",
    "    model,\n",
    "    \n",
    "    random_crop=False,\n",
    "    random_hflip=True,\n",
    "    random_vflip=True,\n",
    "    random_rotate=True,\n",
    "    preserve_aspect_ratio=True\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.ImageFolder(traindir, transforms.Compose([\n",
    "# #         transforms.RandomSizedCrop(max(model.input_size)),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         normalize,\n",
    "#     ])),\n",
    "    datasets.ImageFolder(traindir,train_tf),\n",
    "    batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "\n",
    "val_tf = TransformImage(\n",
    "    model,\n",
    "    \n",
    "    preserve_aspect_ratio=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(valdir,val_tf),\n",
    "    batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch,scheduler):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "#     end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "#         data_time.update(time.time() - end)\n",
    "        input = input.cuda()\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input.float())\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "        #print(input_var.type())\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        #print(output.data)\n",
    "\n",
    "\n",
    "\n",
    "#         # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         # measure elapsed time\n",
    "#         batch_time.update(time.time() - end)\n",
    "#         end = time.time()\n",
    "        meters = trainMeter.update(output,target,loss,input.size(0))\n",
    "\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss:.5f}\\t'\n",
    "                  'Acc {Acc:.5f}\\t'\n",
    "                  'Precision {P:.5f}\\t'\n",
    "                  'Recall {R:.5f}\\t'\n",
    "                  'F1 {F1:.5f}'.format(\n",
    "                   epoch,i, len(train_loader), loss=meters[4],\n",
    "                   Acc=meters[3],P=meters[0],R=meters[1],F1=meters[2]))\n",
    "            \n",
    "            step = epoch*len(train_loader) + i\n",
    "            \n",
    "            writer.add_scalar('TRAIN/Precision', meters[0], step)\n",
    "            writer.add_scalar('TRAIN/Recall', meters[1], step)\n",
    "            writer.add_scalar('TRAIN/F1', meters[2], step)\n",
    "            writer.add_scalar('TRAIN/Acc', meters[3], step)\n",
    "            writer.add_scalar('TRAIN/loss',meters[4], step)\n",
    "            \n",
    "    scheduler.step(meters[4])\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion,epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "#     end = time.time()\n",
    "    meters = []\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda()\n",
    "        input = input.cuda()\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        meters = valMeter.update(output,target,loss,input.size(0))\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Loss {loss:.5f}\\t'\n",
    "                  'Acc {Acc:.5f}\\t'\n",
    "                  'Precision {P:.5f}\\t'\n",
    "                  'Recall {R:.5f}\\t'\n",
    "                  'F1 {F1:.5f}'.format(\n",
    "                   i, len(val_loader), loss=meters[4],\n",
    "                   Acc=meters[3],P=meters[0],R=meters[1],F1=meters[2]))\n",
    "            \n",
    "            step = epoch * len(val_loader) + i\n",
    "            writer.add_scalar('VAL/Precision', meters[0], step)\n",
    "            writer.add_scalar('VAL/Recall', meters[1], step)\n",
    "            writer.add_scalar('VAL/F1', meters[2], step)\n",
    "            writer.add_scalar('VAL/Acc', meters[3], step)\n",
    "            writer.add_scalar('VAL/loss',meters[4], step)\n",
    "    print(' * Acc {Acc:.5f} F1 {F1:.5f}'\n",
    "          .format(Acc=meters[3],F1=meters[2]))\n",
    "    writer.add_scalar('VAL/EPOCH_F1', meters[2], epoch)\n",
    "    return meters[2]\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'tcnn5_model_best.pth.tar')\n",
    "\n",
    "class ModelMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.losses = AverageMeter()\n",
    "        self.top1 = AverageMeter()\n",
    "        self.TP = 1e-8\n",
    "        self.TN = 1e-8\n",
    "        self.FN = 1e-8\n",
    "        self.FP = 1e-8\n",
    "        self.P=1e-8\n",
    "        self.R=1e-8\n",
    "        self.F1=1e-8\n",
    "        self.Acc=1e-8\n",
    "\n",
    "    def update(self, output,target,loss, n=1):\n",
    "        _, pred = output.data.topk(1, 1, True, True)\n",
    "        pred = pred.t()\n",
    "#         print(pred,target.data)\n",
    "        # TP    predict 和 label 同时为1\n",
    "        self.TP += ((pred == 1) & (target.data == 1)).cpu().numpy().sum()\n",
    "        # TN    predict 和 label 同时为0\n",
    "        self.TN += ((pred == 0) & (target.data == 0)).cpu().numpy().sum()\n",
    "        # FN    predict 0 label 1\n",
    "        self.FN += ((pred == 0) & (target.data == 1)).cpu().numpy().sum()\n",
    "        # FP    predict 1 label 0\n",
    "        self.FP += ((pred == 1) & (target.data == 0)).cpu().numpy().sum()\n",
    "#         print(self.TP,self.TN,self.FN,self.FP)\n",
    "#         zes=torch.autograd.Variable(torch.zeros(target.size()).type(torch.cuda.LongTensor))#全0变量\n",
    "\n",
    "#         ons=torch.autograd.Variable(torch.ones(target.size()).type(torch.cuda.LongTensor))#全1变量\n",
    "#         print(zes,ons)\n",
    "#         train_correct01 = ((pred==zes)&(target.data.squeeze(1)==ons)).sum()#原标签为1，预测为 0 的总数\n",
    "\n",
    "#         train_correct10 = ((pred==ons)&(target.data.squeeze(1)==zes)).sum()#原标签为0，预测为1  的总数\n",
    "\n",
    "#         train_correct11 = ((pred_y==ons)&(target.data.squeeze(1)==ons)).sum()\n",
    "#         train_correct00 = ((pred_y==zes)&(target.data.squeeze(1)==zes)).sum()\n",
    "#         self.FN += train_correct01.data[0]        \n",
    "\n",
    "#         self.FP += train_correct10.data[0]\n",
    "\n",
    "#         self.TP += train_correct11.data[0]  \n",
    "#         self.TN += train_correct00.data[0]\n",
    "      \n",
    "        self.P = self.TP / (self.TP + self.FP)\n",
    "        self.R = self.TP / (self.TP + self.FN)\n",
    "        self.F1 = 2 * self.R * self.P / (self.R + self.P)\n",
    "        \n",
    "        self.Acc = (self.TP + self.TN) / (self.TP + self.TN + self.FP + self.FN)\n",
    "        \n",
    "        self.losses.update(loss.data[0],n)\n",
    "\n",
    "        return [self.P,self.R,self.F1,self.Acc,self.losses.avg]\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 1e-8\n",
    "        self.avg = 1e-8\n",
    "        self.sum = 1e-8\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(lr,optimizer, epoch):\n",
    "    if epoch >= 300 and epoch % 100 == 0 :\n",
    "        lr /= 10\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print(\"adjuct learning rate to {}\".format(lr))\n",
    "    return lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# 加载模型，解决命名和维度不匹配问题,解决多个gpu并行\n",
    "def load_state_keywise(model, model_path):\n",
    "    model_dict = model.state_dict()\n",
    "    \n",
    "    print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "    checkpoint = torch.load(model_path,map_location='cpu')\n",
    "    START = checkpoint['epoch']\n",
    "    best_F1 = checkpoint['best_prec1']\n",
    "    #model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    pretrained_dict = checkpoint['state_dict']#torch.load(model_path, map_location='cpu')\n",
    "    key = list(pretrained_dict.keys())[0]\n",
    "    # 1. filter out unnecessary keys\n",
    "    # 1.1 multi-GPU ->CPU\n",
    "    if (str(key).startswith('module.')):\n",
    "        pretrained_dict = {k[7:]: v for k, v in pretrained_dict.items() if\n",
    "                           k[7:] in model_dict and v.size() == model_dict[k[7:]].size()}\n",
    "    else:\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if\n",
    "                           k in model_dict and v.size() == model_dict[k].size()}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(\"start at epoch {}, best_f1={}\".format(START,best_F1))\n",
    "    return model,START,best_F1\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr,momentum = 0.9,weight_decay=1e-6)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, verbose=True)\n",
    "best_f1 = 0\n",
    "model,START,best_f1 = load_state_keywise(model,'tcnn5_checkpoint-Copy1.pth.tar')\n",
    "# model = model.cuda()\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "# TP = 0,TN = 0,FN = 0, FP = 0\n",
    "writer = SummaryWriter()\n",
    "\n",
    "trainMeter = ModelMeter()\n",
    "valMeter = ModelMeter()\n",
    "for epoch in range(START,EPOCHS):\n",
    "    # train for one epoch\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    writer.add_scalar('LR', lr, epoch)\n",
    "    train(train_loader, model, criterion, optimizer, epoch, scheduler)\n",
    "    # evaluate on validation set\n",
    "    F1 = validate(val_loader, model, criterion,epoch)\n",
    "    \n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = F1 > best_f1\n",
    "    best_f1 = max(F1, best_f1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': \"T-CNN\",\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_f1,\n",
    "    }, is_best,filename='tcnn5_checkpoint.pth.tar')\n",
    "# export scalar data to JSON for external processing\n",
    "writer.export_scalars_to_json(\"./test.json\")\n",
    "writer.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.model_zoo as model_zoo\n",
    "# model_dict = model_zoo.load_url('https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth', '/home/krf/model/BALL/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['features.0.weight', 'features.0.bias', 'features.3.weight', 'features.3.bias', 'features.6.weight', 'features.6.bias', 'features.8.weight', 'features.8.bias', 'features.10.weight', 'features.10.bias', 'classifier.1.weight', 'classifier.1.bias', 'classifier.4.weight', 'classifier.4.bias', 'classifier.6.weight', 'classifier.6.bias'])\n"
     ]
    }
   ],
   "source": [
    "# print(model_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'odict_keys' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cb32f4b13958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpre_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'odict_keys' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# model = tcnn(2)\n",
    "# pre_dict = model.state_dict()\n",
    "# print(pre_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k1 = ['features.0.weight', 'features.0.bias', 'features.3.weight', 'features.3.bias', 'features.6.weight', 'features.6.bias', 'features.8.weight', 'features.8.bias', 'features.10.weight', 'features.10.bias', 'classifier.1.weight', 'classifier.1.bias', 'classifier.4.weight', 'classifier.4.bias', 'classifier.6.weight', 'classifier.6.bias']\n",
    "# k2 = ['conv1.0.weight', 'conv1.0.bias', 'conv2.0.weight', 'conv2.0.bias', 'conv3.0.weight', 'conv3.0.bias', 'conv4.0.weight', 'conv4.0.bias', 'conv5.0.weight', 'conv5.0.bias', 'fc1.weight', 'fc1.bias', 'classifier.1.weight', 'classifier.1.bias', 'classifier.3.weight', 'classifier.3.bias']\n",
    "# for i in range(len(k1)):\n",
    "#     pretrained_dict[k2[i]] = model_dict[k1[i]]\n",
    "\n",
    "# # 2. overwrite entries in the existing state dict\n",
    "# model_dict.update(pretrained_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
